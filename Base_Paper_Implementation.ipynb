{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pP5wwA3Fpd-",
        "outputId": "8580ec7d-e371-4319-f566-c4c75a0c0df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BacLABNet: GPU-Accelerated Embedding Extraction\n",
            "======================================================================\n",
            "\n",
            "✓ Device: CPU\n",
            "  ⚠ WARNING: GPU not detected! This will be slow.\n",
            "  → Enable GPU: Runtime → Change runtime type → T4 GPU\n",
            "\n",
            "[1/3] Loading sequences...\n",
            "  Total sequences: 49,964\n",
            "  BacLAB: 24,964, Non-BacLAB: 25,000\n",
            "  Sequence lengths: min=50, max=1996, mean=297.3, median=253.0\n",
            "\n",
            "[2/3] Loading pre-trained RNN model...\n",
            "  ✓ Loaded rnn_gru.pt\n",
            "     Model architecture: embedding(21→10) → GRU(10→128) → decoder(128→21)\n",
            "     Using GRU hidden state (128-dim) as protein embeddings\n",
            "\n",
            "[3/3] Extracting embeddings...\n",
            "Encoding 49964 sequences...\n",
            "  Encoded 10000/49964 sequences...\n",
            "  Encoded 20000/49964 sequences...\n",
            "  Encoded 30000/49964 sequences...\n",
            "  Encoded 40000/49964 sequences...\n",
            "\n",
            "Extracting embeddings on CPU...\n",
            "  Processed 0/49964 sequences...\n",
            "  Processed 640/49964 sequences...\n",
            "  Processed 1280/49964 sequences...\n",
            "  Processed 1920/49964 sequences...\n",
            "  Processed 2560/49964 sequences...\n",
            "  Processed 3200/49964 sequences...\n",
            "  Processed 3840/49964 sequences...\n",
            "  Processed 4480/49964 sequences...\n",
            "  Processed 5120/49964 sequences...\n",
            "  Processed 5760/49964 sequences...\n",
            "  Processed 6400/49964 sequences...\n",
            "  Processed 7040/49964 sequences...\n",
            "  Processed 7680/49964 sequences...\n",
            "  Processed 8320/49964 sequences...\n",
            "  Processed 8960/49964 sequences...\n",
            "  Processed 9600/49964 sequences...\n",
            "  Processed 10240/49964 sequences...\n",
            "  Processed 10880/49964 sequences...\n",
            "  Processed 11520/49964 sequences...\n",
            "  Processed 12160/49964 sequences...\n",
            "  Processed 12800/49964 sequences...\n",
            "  Processed 13440/49964 sequences...\n",
            "  Processed 14080/49964 sequences...\n",
            "  Processed 14720/49964 sequences...\n",
            "  Processed 15360/49964 sequences...\n",
            "  Processed 16000/49964 sequences...\n",
            "  Processed 16640/49964 sequences...\n",
            "  Processed 17280/49964 sequences...\n",
            "  Processed 17920/49964 sequences...\n",
            "  Processed 18560/49964 sequences...\n",
            "  Processed 19200/49964 sequences...\n",
            "  Processed 19840/49964 sequences...\n",
            "  Processed 20480/49964 sequences...\n",
            "  Processed 21120/49964 sequences...\n",
            "  Processed 21760/49964 sequences...\n",
            "  Processed 22400/49964 sequences...\n",
            "  Processed 23040/49964 sequences...\n",
            "  Processed 23680/49964 sequences...\n",
            "  Processed 24320/49964 sequences...\n",
            "  Processed 24960/49964 sequences...\n",
            "  Processed 25600/49964 sequences...\n",
            "  Processed 26240/49964 sequences...\n",
            "  Processed 26880/49964 sequences...\n",
            "  Processed 27520/49964 sequences...\n",
            "  Processed 28160/49964 sequences...\n",
            "  Processed 28800/49964 sequences...\n",
            "  Processed 29440/49964 sequences...\n",
            "  Processed 30080/49964 sequences...\n",
            "  Processed 30720/49964 sequences...\n",
            "  Processed 31360/49964 sequences...\n",
            "  Processed 32000/49964 sequences...\n",
            "  Processed 32640/49964 sequences...\n",
            "  Processed 33280/49964 sequences...\n",
            "  Processed 33920/49964 sequences...\n",
            "  Processed 34560/49964 sequences...\n",
            "  Processed 35200/49964 sequences...\n",
            "  Processed 35840/49964 sequences...\n",
            "  Processed 36480/49964 sequences...\n",
            "  Processed 37120/49964 sequences...\n",
            "  Processed 37760/49964 sequences...\n",
            "  Processed 38400/49964 sequences...\n",
            "  Processed 39040/49964 sequences...\n",
            "  Processed 39680/49964 sequences...\n",
            "  Processed 40320/49964 sequences...\n",
            "  Processed 40960/49964 sequences...\n",
            "  Processed 41600/49964 sequences...\n",
            "  Processed 42240/49964 sequences...\n",
            "  Processed 42880/49964 sequences...\n",
            "  Processed 43520/49964 sequences...\n",
            "  Processed 44160/49964 sequences...\n",
            "  Processed 44800/49964 sequences...\n",
            "  Processed 45440/49964 sequences...\n",
            "  Processed 46080/49964 sequences...\n",
            "  Processed 46720/49964 sequences...\n",
            "  Processed 47360/49964 sequences...\n",
            "  Processed 48000/49964 sequences...\n",
            "  Processed 48640/49964 sequences...\n",
            "  Processed 49280/49964 sequences...\n",
            "  Processed 49920/49964 sequences...\n",
            "\n",
            "✓ Complete!\n",
            "  Shape: (49964, 128)\n",
            "  Time: 164.41 seconds (2.74 minutes)\n",
            "  Speed: 303.9 sequences/second\n",
            "\n",
            "[4/3] Saving embeddings...\n",
            "  ✓ Saved to: embeddings.npy\n",
            "  File size: 24.40 MB\n",
            "\n",
            "======================================================================\n",
            "NEXT STEPS:\n",
            "======================================================================\n",
            "1. Download embeddings.npy from Colab\n",
            "2. Copy to your local project directory\n",
            "3. Run the main training script (it will load pre-computed embeddings)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BacLABNet: Fast Embedding Extraction for Google Colab GPU\n",
        "===========================================================\n",
        "\n",
        "This script extracts ONLY the embedding features using GPU acceleration.\n",
        "Run this on Google Colab with GPU enabled for ~2-5 minutes processing time.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "# ============================================================================\n",
        "# 1. AMINO ACID ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "AMINO_ACIDS = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
        "               'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "\n",
        "AA_TO_IDX = {aa: idx + 1 for idx, aa in enumerate(AMINO_ACIDS)}\n",
        "AA_TO_IDX['X'] = 0  # Unknown amino acid\n",
        "\n",
        "def encode_sequence(sequence: str) -> List[int]:\n",
        "    \"\"\"Encode amino acid sequence to integer indices\"\"\"\n",
        "    return [AA_TO_IDX.get(aa, 0) for aa in sequence.upper()]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. EMBEDDING RNN MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class EmbeddingRNN(nn.Module):\n",
        "    \"\"\"GRU-based RNN for generating embedding vectors (matches rnn_gru.pt architecture)\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 21, embedding_dim: int = 10,\n",
        "                 hidden_dim: int = 128):\n",
        "        super(EmbeddingRNN, self).__init__()\n",
        "\n",
        "        # Architecture from rnn_gru.pt checkpoint:\n",
        "        # - embedding: [21, 10] - small embedding dimension\n",
        "        # - GRU: input=10, hidden=128\n",
        "        # - decoder: [21, 128] - predicts next amino acid (language model)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size)  # Decoder outputs vocab_size (21)\n",
        "\n",
        "    def forward(self, x, return_embedding=True):\n",
        "        # x: (batch, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, 10)\n",
        "        _, hidden = self.gru(embedded)  # hidden: (1, batch, 128)\n",
        "\n",
        "        if return_embedding:\n",
        "            # Return the 128-dim GRU hidden state as embedding\n",
        "            return hidden.squeeze(0)  # (batch, 128)\n",
        "        else:\n",
        "            # Return decoder output for language modeling\n",
        "            output = self.decoder(hidden.squeeze(0))  # (batch, 21)\n",
        "            return output\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. FAST BATCH EMBEDDING EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_embedding_features_gpu(sequences: List[str],\n",
        "                                   embedding_model: EmbeddingRNN,\n",
        "                                   max_len: int = 600,\n",
        "                                   batch_size: int = 128,  # Larger batch for GPU\n",
        "                                   device: str = 'cuda') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract embedding vectors using GPU with large batch processing.\n",
        "\n",
        "    Args:\n",
        "        sequences: List of amino acid sequences\n",
        "        embedding_model: Pre-trained RNN model\n",
        "        max_len: Maximum sequence length (600 covers 99%+ of bacteriocins)\n",
        "        batch_size: 128 for GPU (vs 64 for CPU)\n",
        "        device: 'cuda' for GPU\n",
        "\n",
        "    Returns:\n",
        "        Array of embedding vectors (n_sequences x 128)\n",
        "    \"\"\"\n",
        "    embedding_model.eval()\n",
        "    embedding_model = embedding_model.to(device)\n",
        "    embeddings = []\n",
        "\n",
        "    print(f\"Encoding {len(sequences)} sequences...\")\n",
        "    # Pre-encode and pad all sequences\n",
        "    encoded_sequences = []\n",
        "    for i, seq in enumerate(sequences):\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print(f\"  Encoded {i + 1}/{len(sequences)} sequences...\")\n",
        "        encoded = encode_sequence(seq)\n",
        "        # Pad or truncate to max_len\n",
        "        if len(encoded) < max_len:\n",
        "            encoded = encoded + [0] * (max_len - len(encoded))\n",
        "        else:\n",
        "            encoded = encoded[:max_len]\n",
        "        encoded_sequences.append(encoded)\n",
        "\n",
        "    print(f\"\\nExtracting embeddings on {device.upper()}...\")\n",
        "    # Process in batches\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(encoded_sequences), batch_size):\n",
        "            if i % (batch_size * 10) == 0:\n",
        "                print(f\"  Processed {i}/{len(encoded_sequences)} sequences...\")\n",
        "            batch = encoded_sequences[i:i + batch_size]\n",
        "            batch_tensor = torch.LongTensor(batch).to(device)\n",
        "            batch_embeddings = embedding_model(batch_tensor).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. MAIN SCRIPT\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"BacLABNet: GPU-Accelerated Embedding Extraction\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Check GPU availability\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"\\n✓ Device: {device.upper()}\")\n",
        "    if device == 'cpu':\n",
        "        print(\"  ⚠ WARNING: GPU not detected! This will be slow.\")\n",
        "        print(\"  → Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
        "    else:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"  GPU: {gpu_name}\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n[1/3] Loading sequences...\")\n",
        "    df = pd.read_csv('data_BacLAB_and_nonBacLAB.csv',\n",
        "                     header=None,\n",
        "                     names=['ID', 'Species', 'Sequence', 'Label', 'Empty'])\n",
        "\n",
        "    sequences = df['Sequence'].tolist()\n",
        "    labels = df['Label'].values\n",
        "\n",
        "    print(f\"  Total sequences: {len(sequences):,}\")\n",
        "    print(f\"  BacLAB: {sum(labels):,}, Non-BacLAB: {len(labels) - sum(labels):,}\")\n",
        "\n",
        "    # Calculate sequence length statistics\n",
        "    seq_lengths = [len(seq) for seq in sequences]\n",
        "    print(f\"  Sequence lengths: min={min(seq_lengths)}, max={max(seq_lengths)}, \"\n",
        "          f\"mean={np.mean(seq_lengths):.1f}, median={np.median(seq_lengths):.1f}\")\n",
        "\n",
        "    # Load pre-trained model\n",
        "    print(\"\\n[2/3] Loading pre-trained RNN model...\")\n",
        "    embedding_model = EmbeddingRNN(vocab_size=21, embedding_dim=10, hidden_dim=128)\n",
        "\n",
        "    try:\n",
        "        state_dict = torch.load('rnn_gru.pt', map_location=device)\n",
        "        embedding_model.load_state_dict(state_dict)\n",
        "        print(\"  ✓ Loaded rnn_gru.pt\")\n",
        "        print(\"     Model architecture: embedding(21→10) → GRU(10→128) → decoder(128→21)\")\n",
        "        print(\"     Using GRU hidden state (128-dim) as protein embeddings\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Extract embeddings\n",
        "    print(\"\\n[3/3] Extracting embeddings...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    embedding_features = extract_embedding_features_gpu(\n",
        "        sequences,\n",
        "        embedding_model,\n",
        "        max_len=600,\n",
        "        batch_size=128 if device == 'cuda' else 64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n✓ Complete!\")\n",
        "    print(f\"  Shape: {embedding_features.shape}\")\n",
        "    print(f\"  Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
        "    print(f\"  Speed: {len(sequences)/elapsed_time:.1f} sequences/second\")\n",
        "\n",
        "    # Save embeddings\n",
        "    print(\"\\n[4/3] Saving embeddings...\")\n",
        "    np.save('embeddings.npy', embedding_features)\n",
        "    print(\"  ✓ Saved to: embeddings.npy\")\n",
        "    print(f\"  File size: {embedding_features.nbytes / (1024**2):.2f} MB\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"NEXT STEPS:\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"1. Download embeddings.npy from Colab\")\n",
        "    print(\"2. Copy to your local project directory\")\n",
        "    print(\"3. Run the main training script (it will load pre-computed embeddings)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvWmuSBqJp8X",
        "outputId": "3c8d6f50-919c-493d-d33c-2444197bd9d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BacLABNet: Bacteriocin Classification using Deep Learning\n",
            "======================================================================\n",
            "\n",
            "[1/7] Loading data...\n",
            "Total sequences: 49964\n",
            "BacLAB: 24964, Non-BacLAB: 25000\n",
            "\n",
            "[2/7] Extracting k-mer features...\n",
            "Computing k-mers from BacLAB sequences...\n",
            "5-mer features shape: (49964, 100)\n",
            "7-mer features shape: (49964, 100)\n",
            "\n",
            "[3/7] Extracting embedding features...\n",
            "✓ Loaded pre-computed embeddings from 'embeddings.npy'\n",
            "  Shape: (49964, 128)\n",
            "  (Skipping embedding extraction - using cached results)\n",
            "\n",
            "[4/7] Concatenating features...\n",
            "Final features shape: (49964, 328)\n",
            "Input dimension: 328 (100 + 100 + 128 = 328)\n",
            "\n",
            "[5/7] Starting k-fold cross-validation (k=30)...\n",
            "\n",
            "============================================================\n",
            "Fold 1/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5093, Val Loss: 0.4916, Val Acc: 0.8241\n",
            "Epoch 20/75 - Train Loss: 0.4908, Val Loss: 0.4759, Val Acc: 0.8361\n",
            "Epoch 30/75 - Train Loss: 0.4839, Val Loss: 0.4749, Val Acc: 0.8325\n",
            "Epoch 40/75 - Train Loss: 0.4788, Val Loss: 0.4697, Val Acc: 0.8385\n",
            "Epoch 50/75 - Train Loss: 0.4773, Val Loss: 0.4687, Val Acc: 0.8385\n",
            "Epoch 60/75 - Train Loss: 0.4758, Val Loss: 0.4684, Val Acc: 0.8403\n",
            "Epoch 70/75 - Train Loss: 0.4738, Val Loss: 0.4663, Val Acc: 0.8481\n",
            "\n",
            "Fold 1 Results:\n",
            "Loss: 46.34%\n",
            "Accuracy: 85.05%\n",
            "Precision: 0.8062\n",
            "Recall: 0.9187\n",
            "F1 Score: 0.8588\n",
            "\n",
            "============================================================\n",
            "Fold 2/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5078, Val Loss: 0.4783, Val Acc: 0.8355\n",
            "Epoch 20/75 - Train Loss: 0.4905, Val Loss: 0.4627, Val Acc: 0.8481\n",
            "Epoch 30/75 - Train Loss: 0.4838, Val Loss: 0.4549, Val Acc: 0.8559\n",
            "Epoch 40/75 - Train Loss: 0.4803, Val Loss: 0.4534, Val Acc: 0.8571\n",
            "Epoch 50/75 - Train Loss: 0.4781, Val Loss: 0.4507, Val Acc: 0.8601\n",
            "Epoch 60/75 - Train Loss: 0.4756, Val Loss: 0.4513, Val Acc: 0.8559\n",
            "Epoch 70/75 - Train Loss: 0.4738, Val Loss: 0.4516, Val Acc: 0.8607\n",
            "\n",
            "Fold 2 Results:\n",
            "Loss: 45.00%\n",
            "Accuracy: 85.95%\n",
            "Precision: 0.8315\n",
            "Recall: 0.9014\n",
            "F1 Score: 0.8651\n",
            "\n",
            "============================================================\n",
            "Fold 3/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5098, Val Loss: 0.4884, Val Acc: 0.8277\n",
            "Epoch 20/75 - Train Loss: 0.4898, Val Loss: 0.4697, Val Acc: 0.8403\n",
            "Epoch 30/75 - Train Loss: 0.4836, Val Loss: 0.4641, Val Acc: 0.8427\n",
            "Epoch 40/75 - Train Loss: 0.4813, Val Loss: 0.4606, Val Acc: 0.8445\n",
            "Epoch 50/75 - Train Loss: 0.4786, Val Loss: 0.4620, Val Acc: 0.8427\n",
            "Epoch 60/75 - Train Loss: 0.4760, Val Loss: 0.4588, Val Acc: 0.8487\n",
            "Epoch 70/75 - Train Loss: 0.4746, Val Loss: 0.4584, Val Acc: 0.8463\n",
            "\n",
            "Fold 3 Results:\n",
            "Loss: 45.84%\n",
            "Accuracy: 85.05%\n",
            "Precision: 0.8067\n",
            "Recall: 0.9322\n",
            "F1 Score: 0.8649\n",
            "\n",
            "============================================================\n",
            "Fold 4/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5093, Val Loss: 0.5045, Val Acc: 0.8025\n",
            "Epoch 20/75 - Train Loss: 0.4910, Val Loss: 0.4865, Val Acc: 0.8193\n",
            "Epoch 30/75 - Train Loss: 0.4833, Val Loss: 0.4795, Val Acc: 0.8307\n",
            "Epoch 40/75 - Train Loss: 0.4802, Val Loss: 0.4758, Val Acc: 0.8349\n",
            "Epoch 50/75 - Train Loss: 0.4769, Val Loss: 0.4721, Val Acc: 0.8385\n",
            "Epoch 60/75 - Train Loss: 0.4745, Val Loss: 0.4730, Val Acc: 0.8355\n",
            "Epoch 70/75 - Train Loss: 0.4744, Val Loss: 0.4741, Val Acc: 0.8331\n",
            "\n",
            "Fold 4 Results:\n",
            "Loss: 47.24%\n",
            "Accuracy: 83.73%\n",
            "Precision: 0.7869\n",
            "Recall: 0.9198\n",
            "F1 Score: 0.8482\n",
            "\n",
            "============================================================\n",
            "Fold 5/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5099, Val Loss: 0.4989, Val Acc: 0.8025\n",
            "Epoch 20/75 - Train Loss: 0.4914, Val Loss: 0.4759, Val Acc: 0.8337\n",
            "Epoch 30/75 - Train Loss: 0.4830, Val Loss: 0.4730, Val Acc: 0.8349\n",
            "Epoch 40/75 - Train Loss: 0.4797, Val Loss: 0.4723, Val Acc: 0.8343\n",
            "Epoch 50/75 - Train Loss: 0.4780, Val Loss: 0.4727, Val Acc: 0.8349\n",
            "Epoch 60/75 - Train Loss: 0.4756, Val Loss: 0.4713, Val Acc: 0.8379\n",
            "Epoch 70/75 - Train Loss: 0.4755, Val Loss: 0.4663, Val Acc: 0.8451\n",
            "\n",
            "Fold 5 Results:\n",
            "Loss: 46.66%\n",
            "Accuracy: 84.51%\n",
            "Precision: 0.7928\n",
            "Recall: 0.9463\n",
            "F1 Score: 0.8628\n",
            "\n",
            "============================================================\n",
            "Fold 6/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5114, Val Loss: 0.4942, Val Acc: 0.8193\n",
            "Epoch 20/75 - Train Loss: 0.4909, Val Loss: 0.4765, Val Acc: 0.8313\n",
            "Epoch 30/75 - Train Loss: 0.4835, Val Loss: 0.4720, Val Acc: 0.8361\n",
            "Epoch 40/75 - Train Loss: 0.4806, Val Loss: 0.4705, Val Acc: 0.8409\n",
            "Epoch 50/75 - Train Loss: 0.4789, Val Loss: 0.4682, Val Acc: 0.8409\n",
            "Epoch 60/75 - Train Loss: 0.4762, Val Loss: 0.4684, Val Acc: 0.8403\n",
            "Epoch 70/75 - Train Loss: 0.4739, Val Loss: 0.4691, Val Acc: 0.8391\n",
            "\n",
            "Fold 6 Results:\n",
            "Loss: 46.83%\n",
            "Accuracy: 83.91%\n",
            "Precision: 0.7896\n",
            "Recall: 0.9199\n",
            "F1 Score: 0.8498\n",
            "\n",
            "============================================================\n",
            "Fold 7/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5098, Val Loss: 0.4856, Val Acc: 0.8271\n",
            "Epoch 20/75 - Train Loss: 0.4902, Val Loss: 0.4714, Val Acc: 0.8331\n",
            "Epoch 30/75 - Train Loss: 0.4831, Val Loss: 0.4728, Val Acc: 0.8319\n",
            "Epoch 40/75 - Train Loss: 0.4791, Val Loss: 0.4679, Val Acc: 0.8355\n",
            "Epoch 50/75 - Train Loss: 0.4773, Val Loss: 0.4676, Val Acc: 0.8397\n",
            "Epoch 60/75 - Train Loss: 0.4749, Val Loss: 0.4624, Val Acc: 0.8445\n",
            "Epoch 70/75 - Train Loss: 0.4734, Val Loss: 0.4628, Val Acc: 0.8463\n",
            "\n",
            "Fold 7 Results:\n",
            "Loss: 46.34%\n",
            "Accuracy: 84.09%\n",
            "Precision: 0.8033\n",
            "Recall: 0.8947\n",
            "F1 Score: 0.8466\n",
            "\n",
            "============================================================\n",
            "Fold 8/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5121, Val Loss: 0.4903, Val Acc: 0.8181\n",
            "Epoch 20/75 - Train Loss: 0.4938, Val Loss: 0.4734, Val Acc: 0.8355\n",
            "Epoch 30/75 - Train Loss: 0.4859, Val Loss: 0.4676, Val Acc: 0.8415\n",
            "Epoch 40/75 - Train Loss: 0.4816, Val Loss: 0.4665, Val Acc: 0.8421\n",
            "Epoch 50/75 - Train Loss: 0.4780, Val Loss: 0.4646, Val Acc: 0.8421\n",
            "Epoch 60/75 - Train Loss: 0.4767, Val Loss: 0.4630, Val Acc: 0.8475\n",
            "Epoch 70/75 - Train Loss: 0.4760, Val Loss: 0.4623, Val Acc: 0.8493\n",
            "\n",
            "Fold 8 Results:\n",
            "Loss: 46.01%\n",
            "Accuracy: 84.87%\n",
            "Precision: 0.8035\n",
            "Recall: 0.9276\n",
            "F1 Score: 0.8611\n",
            "\n",
            "============================================================\n",
            "Fold 9/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5091, Val Loss: 0.4908, Val Acc: 0.8259\n",
            "Epoch 20/75 - Train Loss: 0.4904, Val Loss: 0.4717, Val Acc: 0.8385\n",
            "Epoch 30/75 - Train Loss: 0.4832, Val Loss: 0.4680, Val Acc: 0.8451\n",
            "Epoch 40/75 - Train Loss: 0.4795, Val Loss: 0.4649, Val Acc: 0.8481\n",
            "Epoch 50/75 - Train Loss: 0.4768, Val Loss: 0.4640, Val Acc: 0.8475\n",
            "Epoch 60/75 - Train Loss: 0.4752, Val Loss: 0.4629, Val Acc: 0.8523\n",
            "Epoch 70/75 - Train Loss: 0.4739, Val Loss: 0.4621, Val Acc: 0.8505\n",
            "\n",
            "Fold 9 Results:\n",
            "Loss: 46.38%\n",
            "Accuracy: 84.63%\n",
            "Precision: 0.7918\n",
            "Recall: 0.9300\n",
            "F1 Score: 0.8554\n",
            "\n",
            "============================================================\n",
            "Fold 10/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5122, Val Loss: 0.4960, Val Acc: 0.8169\n",
            "Epoch 20/75 - Train Loss: 0.4927, Val Loss: 0.4735, Val Acc: 0.8343\n",
            "Epoch 30/75 - Train Loss: 0.4856, Val Loss: 0.4687, Val Acc: 0.8403\n",
            "Epoch 40/75 - Train Loss: 0.4824, Val Loss: 0.4637, Val Acc: 0.8463\n",
            "Epoch 50/75 - Train Loss: 0.4781, Val Loss: 0.4653, Val Acc: 0.8427\n",
            "Epoch 60/75 - Train Loss: 0.4763, Val Loss: 0.4626, Val Acc: 0.8499\n",
            "Epoch 70/75 - Train Loss: 0.4751, Val Loss: 0.4617, Val Acc: 0.8463\n",
            "\n",
            "Fold 10 Results:\n",
            "Loss: 46.39%\n",
            "Accuracy: 84.45%\n",
            "Precision: 0.8115\n",
            "Recall: 0.9075\n",
            "F1 Score: 0.8568\n",
            "\n",
            "============================================================\n",
            "Fold 11/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5125, Val Loss: 0.4847, Val Acc: 0.8325\n",
            "Epoch 20/75 - Train Loss: 0.4914, Val Loss: 0.4678, Val Acc: 0.8421\n",
            "Epoch 30/75 - Train Loss: 0.4851, Val Loss: 0.4618, Val Acc: 0.8505\n",
            "Epoch 40/75 - Train Loss: 0.4799, Val Loss: 0.4567, Val Acc: 0.8535\n",
            "Epoch 50/75 - Train Loss: 0.4787, Val Loss: 0.4573, Val Acc: 0.8541\n",
            "Epoch 60/75 - Train Loss: 0.4761, Val Loss: 0.4528, Val Acc: 0.8541\n",
            "Epoch 70/75 - Train Loss: 0.4748, Val Loss: 0.4519, Val Acc: 0.8601\n",
            "\n",
            "Fold 11 Results:\n",
            "Loss: 45.29%\n",
            "Accuracy: 85.59%\n",
            "Precision: 0.8150\n",
            "Recall: 0.9253\n",
            "F1 Score: 0.8667\n",
            "\n",
            "============================================================\n",
            "Fold 12/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5102, Val Loss: 0.4800, Val Acc: 0.8349\n",
            "Epoch 20/75 - Train Loss: 0.4899, Val Loss: 0.4648, Val Acc: 0.8445\n",
            "Epoch 30/75 - Train Loss: 0.4849, Val Loss: 0.4594, Val Acc: 0.8523\n",
            "Epoch 40/75 - Train Loss: 0.4804, Val Loss: 0.4602, Val Acc: 0.8517\n",
            "Epoch 50/75 - Train Loss: 0.4775, Val Loss: 0.4580, Val Acc: 0.8541\n",
            "Epoch 60/75 - Train Loss: 0.4771, Val Loss: 0.4559, Val Acc: 0.8529\n",
            "Epoch 70/75 - Train Loss: 0.4747, Val Loss: 0.4571, Val Acc: 0.8529\n",
            "\n",
            "Fold 12 Results:\n",
            "Loss: 45.56%\n",
            "Accuracy: 85.53%\n",
            "Precision: 0.8092\n",
            "Recall: 0.9183\n",
            "F1 Score: 0.8603\n",
            "\n",
            "============================================================\n",
            "Fold 13/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5077, Val Loss: 0.4876, Val Acc: 0.8235\n",
            "Epoch 20/75 - Train Loss: 0.4896, Val Loss: 0.4767, Val Acc: 0.8313\n",
            "Epoch 30/75 - Train Loss: 0.4824, Val Loss: 0.4710, Val Acc: 0.8325\n",
            "Epoch 40/75 - Train Loss: 0.4804, Val Loss: 0.4701, Val Acc: 0.8367\n",
            "Epoch 50/75 - Train Loss: 0.4785, Val Loss: 0.4707, Val Acc: 0.8361\n",
            "Epoch 60/75 - Train Loss: 0.4756, Val Loss: 0.4688, Val Acc: 0.8367\n",
            "Epoch 70/75 - Train Loss: 0.4748, Val Loss: 0.4685, Val Acc: 0.8379\n",
            "\n",
            "Fold 13 Results:\n",
            "Loss: 46.28%\n",
            "Accuracy: 84.21%\n",
            "Precision: 0.7961\n",
            "Recall: 0.9210\n",
            "F1 Score: 0.8540\n",
            "\n",
            "============================================================\n",
            "Fold 14/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5137, Val Loss: 0.4807, Val Acc: 0.8403\n",
            "Epoch 20/75 - Train Loss: 0.4925, Val Loss: 0.4637, Val Acc: 0.8481\n",
            "Epoch 30/75 - Train Loss: 0.4855, Val Loss: 0.4632, Val Acc: 0.8427\n",
            "Epoch 40/75 - Train Loss: 0.4808, Val Loss: 0.4545, Val Acc: 0.8541\n",
            "Epoch 50/75 - Train Loss: 0.4795, Val Loss: 0.4531, Val Acc: 0.8553\n",
            "Epoch 60/75 - Train Loss: 0.4762, Val Loss: 0.4520, Val Acc: 0.8565\n",
            "Epoch 70/75 - Train Loss: 0.4759, Val Loss: 0.4479, Val Acc: 0.8595\n",
            "\n",
            "Fold 14 Results:\n",
            "Loss: 44.90%\n",
            "Accuracy: 85.53%\n",
            "Precision: 0.8090\n",
            "Recall: 0.9167\n",
            "F1 Score: 0.8595\n",
            "\n",
            "============================================================\n",
            "Fold 15/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5122, Val Loss: 0.4947, Val Acc: 0.8162\n",
            "Epoch 20/75 - Train Loss: 0.4932, Val Loss: 0.4779, Val Acc: 0.8324\n",
            "Epoch 30/75 - Train Loss: 0.4857, Val Loss: 0.4718, Val Acc: 0.8348\n",
            "Epoch 40/75 - Train Loss: 0.4813, Val Loss: 0.4680, Val Acc: 0.8402\n",
            "Epoch 50/75 - Train Loss: 0.4782, Val Loss: 0.4670, Val Acc: 0.8414\n",
            "Epoch 60/75 - Train Loss: 0.4771, Val Loss: 0.4657, Val Acc: 0.8390\n",
            "Epoch 70/75 - Train Loss: 0.4745, Val Loss: 0.4661, Val Acc: 0.8390\n",
            "\n",
            "Fold 15 Results:\n",
            "Loss: 46.70%\n",
            "Accuracy: 84.14%\n",
            "Precision: 0.7892\n",
            "Recall: 0.9152\n",
            "F1 Score: 0.8476\n",
            "\n",
            "============================================================\n",
            "Fold 16/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5093, Val Loss: 0.4829, Val Acc: 0.8360\n",
            "Epoch 20/75 - Train Loss: 0.4900, Val Loss: 0.4691, Val Acc: 0.8384\n",
            "Epoch 30/75 - Train Loss: 0.4854, Val Loss: 0.4646, Val Acc: 0.8456\n",
            "Epoch 40/75 - Train Loss: 0.4810, Val Loss: 0.4639, Val Acc: 0.8450\n",
            "Epoch 50/75 - Train Loss: 0.4773, Val Loss: 0.4642, Val Acc: 0.8450\n",
            "Epoch 60/75 - Train Loss: 0.4767, Val Loss: 0.4654, Val Acc: 0.8432\n",
            "Epoch 70/75 - Train Loss: 0.4746, Val Loss: 0.4612, Val Acc: 0.8486\n",
            "\n",
            "Fold 16 Results:\n",
            "Loss: 46.24%\n",
            "Accuracy: 84.86%\n",
            "Precision: 0.8050\n",
            "Recall: 0.9101\n",
            "F1 Score: 0.8543\n",
            "\n",
            "============================================================\n",
            "Fold 17/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5088, Val Loss: 0.4767, Val Acc: 0.8336\n",
            "Epoch 20/75 - Train Loss: 0.4918, Val Loss: 0.4661, Val Acc: 0.8444\n",
            "Epoch 30/75 - Train Loss: 0.4837, Val Loss: 0.4571, Val Acc: 0.8517\n",
            "Epoch 40/75 - Train Loss: 0.4802, Val Loss: 0.4549, Val Acc: 0.8535\n",
            "Epoch 50/75 - Train Loss: 0.4773, Val Loss: 0.4526, Val Acc: 0.8535\n",
            "Epoch 60/75 - Train Loss: 0.4747, Val Loss: 0.4529, Val Acc: 0.8535\n",
            "Epoch 70/75 - Train Loss: 0.4732, Val Loss: 0.4514, Val Acc: 0.8517\n",
            "\n",
            "Fold 17 Results:\n",
            "Loss: 45.37%\n",
            "Accuracy: 85.59%\n",
            "Precision: 0.8165\n",
            "Recall: 0.9169\n",
            "F1 Score: 0.8638\n",
            "\n",
            "============================================================\n",
            "Fold 18/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5094, Val Loss: 0.4721, Val Acc: 0.8432\n",
            "Epoch 20/75 - Train Loss: 0.4905, Val Loss: 0.4555, Val Acc: 0.8565\n",
            "Epoch 30/75 - Train Loss: 0.4852, Val Loss: 0.4548, Val Acc: 0.8547\n",
            "Epoch 40/75 - Train Loss: 0.4800, Val Loss: 0.4498, Val Acc: 0.8607\n",
            "Epoch 50/75 - Train Loss: 0.4788, Val Loss: 0.4487, Val Acc: 0.8631\n",
            "Epoch 60/75 - Train Loss: 0.4758, Val Loss: 0.4474, Val Acc: 0.8601\n",
            "Epoch 70/75 - Train Loss: 0.4752, Val Loss: 0.4463, Val Acc: 0.8625\n",
            "\n",
            "Fold 18 Results:\n",
            "Loss: 44.97%\n",
            "Accuracy: 85.71%\n",
            "Precision: 0.8086\n",
            "Recall: 0.9408\n",
            "F1 Score: 0.8697\n",
            "\n",
            "============================================================\n",
            "Fold 19/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5127, Val Loss: 0.4728, Val Acc: 0.8498\n",
            "Epoch 20/75 - Train Loss: 0.4937, Val Loss: 0.4566, Val Acc: 0.8583\n",
            "Epoch 30/75 - Train Loss: 0.4859, Val Loss: 0.4498, Val Acc: 0.8625\n",
            "Epoch 40/75 - Train Loss: 0.4809, Val Loss: 0.4477, Val Acc: 0.8637\n",
            "Epoch 50/75 - Train Loss: 0.4787, Val Loss: 0.4455, Val Acc: 0.8673\n",
            "Epoch 60/75 - Train Loss: 0.4779, Val Loss: 0.4457, Val Acc: 0.8655\n",
            "Epoch 70/75 - Train Loss: 0.4749, Val Loss: 0.4446, Val Acc: 0.8625\n",
            "\n",
            "Fold 19 Results:\n",
            "Loss: 44.28%\n",
            "Accuracy: 86.61%\n",
            "Precision: 0.8187\n",
            "Recall: 0.9413\n",
            "F1 Score: 0.8758\n",
            "\n",
            "============================================================\n",
            "Fold 20/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5111, Val Loss: 0.4908, Val Acc: 0.8192\n",
            "Epoch 20/75 - Train Loss: 0.4930, Val Loss: 0.4766, Val Acc: 0.8288\n",
            "Epoch 30/75 - Train Loss: 0.4855, Val Loss: 0.4680, Val Acc: 0.8402\n",
            "Epoch 40/75 - Train Loss: 0.4805, Val Loss: 0.4682, Val Acc: 0.8390\n",
            "Epoch 50/75 - Train Loss: 0.4793, Val Loss: 0.4649, Val Acc: 0.8420\n",
            "Epoch 60/75 - Train Loss: 0.4754, Val Loss: 0.4629, Val Acc: 0.8420\n",
            "Epoch 70/75 - Train Loss: 0.4755, Val Loss: 0.4623, Val Acc: 0.8474\n",
            "\n",
            "Fold 20 Results:\n",
            "Loss: 46.43%\n",
            "Accuracy: 84.62%\n",
            "Precision: 0.8051\n",
            "Recall: 0.9119\n",
            "F1 Score: 0.8552\n",
            "\n",
            "============================================================\n",
            "Fold 21/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5093, Val Loss: 0.4758, Val Acc: 0.8372\n",
            "Epoch 20/75 - Train Loss: 0.4890, Val Loss: 0.4589, Val Acc: 0.8498\n",
            "Epoch 30/75 - Train Loss: 0.4842, Val Loss: 0.4547, Val Acc: 0.8535\n",
            "Epoch 40/75 - Train Loss: 0.4809, Val Loss: 0.4538, Val Acc: 0.8529\n",
            "Epoch 50/75 - Train Loss: 0.4777, Val Loss: 0.4506, Val Acc: 0.8589\n",
            "Epoch 60/75 - Train Loss: 0.4764, Val Loss: 0.4496, Val Acc: 0.8583\n",
            "Epoch 70/75 - Train Loss: 0.4732, Val Loss: 0.4456, Val Acc: 0.8613\n",
            "\n",
            "Fold 21 Results:\n",
            "Loss: 44.78%\n",
            "Accuracy: 86.07%\n",
            "Precision: 0.8285\n",
            "Recall: 0.9034\n",
            "F1 Score: 0.8643\n",
            "\n",
            "============================================================\n",
            "Fold 22/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5126, Val Loss: 0.4844, Val Acc: 0.8294\n",
            "Epoch 20/75 - Train Loss: 0.4928, Val Loss: 0.4632, Val Acc: 0.8462\n",
            "Epoch 30/75 - Train Loss: 0.4857, Val Loss: 0.4587, Val Acc: 0.8505\n",
            "Epoch 40/75 - Train Loss: 0.4817, Val Loss: 0.4545, Val Acc: 0.8547\n",
            "Epoch 50/75 - Train Loss: 0.4770, Val Loss: 0.4548, Val Acc: 0.8535\n",
            "Epoch 60/75 - Train Loss: 0.4767, Val Loss: 0.4517, Val Acc: 0.8565\n",
            "Epoch 70/75 - Train Loss: 0.4740, Val Loss: 0.4515, Val Acc: 0.8571\n",
            "\n",
            "Fold 22 Results:\n",
            "Loss: 45.01%\n",
            "Accuracy: 85.47%\n",
            "Precision: 0.8071\n",
            "Recall: 0.9386\n",
            "F1 Score: 0.8679\n",
            "\n",
            "============================================================\n",
            "Fold 23/30\n",
            "============================================================\n",
            "Epoch 10/75 - Train Loss: 0.5134, Val Loss: 0.4894, Val Acc: 0.8210\n",
            "Epoch 20/75 - Train Loss: 0.4927, Val Loss: 0.4680, Val Acc: 0.8450\n",
            "Epoch 30/75 - Train Loss: 0.4867, Val Loss: 0.4640, Val Acc: 0.8438\n",
            "Epoch 40/75 - Train Loss: 0.4799, Val Loss: 0.4566, Val Acc: 0.8523\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BacLABNet: Deep Learning Neural Network for Bacteriocin Classification\n",
        "Reproduces the methodology from González et al. (2025)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Dict\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# 1. AMINO ACID ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "AMINO_ACIDS = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
        "               'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "\n",
        "AA_TO_IDX = {aa: idx + 1 for idx, aa in enumerate(AMINO_ACIDS)}\n",
        "AA_TO_IDX['X'] = 0  # Unknown amino acid\n",
        "\n",
        "def encode_sequence(sequence: str) -> List[int]:\n",
        "    \"\"\"Encode amino acid sequence to integer indices\"\"\"\n",
        "    return [AA_TO_IDX.get(aa, 0) for aa in sequence.upper()]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. K-MER FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_kmers(sequence: str, k: int) -> List[str]:\n",
        "    \"\"\"Generate all k-mers from a sequence\"\"\"\n",
        "    sequence = sequence.upper()\n",
        "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
        "\n",
        "def get_top_kmers(sequences: List[str], k: int, top_n: int = 100) -> List[str]:\n",
        "    \"\"\"Get top N most frequent k-mers from sequences\"\"\"\n",
        "    kmer_counts = {}\n",
        "\n",
        "    for seq in sequences:\n",
        "        kmers = generate_kmers(seq, k)\n",
        "        for kmer in kmers:\n",
        "            kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n",
        "\n",
        "    # Sort by frequency and get top N\n",
        "    sorted_kmers = sorted(kmer_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [kmer for kmer, count in sorted_kmers[:top_n]]\n",
        "\n",
        "def extract_kmer_features(sequence: str, kmer_list: List[str], k: int) -> np.ndarray:\n",
        "    \"\"\"Extract binary k-mer features (presence/absence)\"\"\"\n",
        "    seq_kmers = set(generate_kmers(sequence, k))\n",
        "    features = np.array([1 if kmer in seq_kmers else 0 for kmer in kmer_list])\n",
        "    return features\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. EMBEDDING VECTOR EXTRACTION (GRU-BASED RNN)\n",
        "# ============================================================================\n",
        "\n",
        "class EmbeddingRNN(nn.Module):\n",
        "    \"\"\"GRU-based RNN for generating embedding vectors\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 21, embedding_dim: int = 128,\n",
        "                 hidden_dim: int = 128):\n",
        "        super(EmbeddingRNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
        "        _, hidden = self.gru(embedded)  # hidden: (1, batch, hidden_dim)\n",
        "        output = self.linear(hidden.squeeze(0))  # (batch, embedding_dim)\n",
        "        return output\n",
        "\n",
        "def extract_embedding_features(sequences: List[str],\n",
        "                               embedding_model: EmbeddingRNN,\n",
        "                               max_len: int = 600,\n",
        "                               batch_size: int = 64,\n",
        "                               device: str = 'cpu') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract embedding vectors using pre-trained RNN with batch processing.\n",
        "\n",
        "    Args:\n",
        "        sequences: List of amino acid sequences\n",
        "        embedding_model: Pre-trained RNN model\n",
        "        max_len: Maximum sequence length (reduced from 2000 to 600 for speed)\n",
        "        batch_size: Number of sequences to process at once (default 64)\n",
        "        device: Device to run model on ('cpu' or 'cuda')\n",
        "\n",
        "    Returns:\n",
        "        Array of embedding vectors (n_sequences x embedding_dim)\n",
        "    \"\"\"\n",
        "    embedding_model.eval()\n",
        "    embedding_model = embedding_model.to(device)\n",
        "    embeddings = []\n",
        "\n",
        "    # Pre-encode and pad all sequences\n",
        "    encoded_sequences = []\n",
        "    for seq in sequences:\n",
        "        encoded = encode_sequence(seq)\n",
        "        # Pad or truncate to max_len\n",
        "        if len(encoded) < max_len:\n",
        "            encoded = encoded + [0] * (max_len - len(encoded))\n",
        "        else:\n",
        "            encoded = encoded[:max_len]\n",
        "        encoded_sequences.append(encoded)\n",
        "\n",
        "    # Process in batches for 6-12x speedup\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(encoded_sequences), batch_size):\n",
        "            batch = encoded_sequences[i:i + batch_size]\n",
        "            # Convert batch to tensor\n",
        "            batch_tensor = torch.LongTensor(batch).to(device)\n",
        "            # Get embeddings for entire batch at once\n",
        "            batch_embeddings = embedding_model(batch_tensor).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. DEEP NEURAL NETWORK ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class BacteriocinClassifierDNN(nn.Module):\n",
        "    \"\"\"Deep Neural Network for Bacteriocin Classification\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int):\n",
        "        super(BacteriocinClassifierDNN, self).__init__()\n",
        "\n",
        "        # Block 1: 128 neurons\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "\n",
        "        # Block 2: 64 neurons\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Block 3: 32 neurons\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Block 4: Output (2 neurons)\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class BacteriocinDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for bacteriocin sequences\"\"\"\n",
        "\n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for features, labels in dataloader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='binary')\n",
        "    recall = recall_score(all_labels, all_preds, average='binary')\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. K-FOLD CROSS VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "def kfold_cross_validation(features: np.ndarray,\n",
        "                          labels: np.ndarray,\n",
        "                          k: int = 30,\n",
        "                          epochs: int = 75,\n",
        "                          batch_size: int = 40,\n",
        "                          learning_rate: float = 2.5e-5,\n",
        "                          device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Perform k-fold cross validation\"\"\"\n",
        "\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(features)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Fold {fold + 1}/{k}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val = features[train_idx], features[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_dataset = BacteriocinDataset(X_train, y_train)\n",
        "        val_dataset = BacteriocinDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = BacteriocinClassifierDNN(input_dim=features.shape[1]).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Training history\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_loss, val_acc, val_prec, val_rec, val_f1, _, _ = evaluate(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Final evaluation\n",
        "        final_loss, final_acc, final_prec, final_rec, final_f1, preds, true_labels = evaluate(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'loss': final_loss * 100,  # Convert to percentage\n",
        "            'accuracy': final_acc * 100,\n",
        "            'precision': final_prec,\n",
        "            'recall': final_rec,\n",
        "            'f1_score': final_f1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_accuracies': val_accuracies,\n",
        "            'predictions': preds,\n",
        "            'true_labels': true_labels,\n",
        "            'model_state': model.state_dict()\n",
        "        })\n",
        "\n",
        "        print(f\"\\nFold {fold + 1} Results:\")\n",
        "        print(f\"Loss: {final_loss * 100:.2f}%\")\n",
        "        print(f\"Accuracy: {final_acc * 100:.2f}%\")\n",
        "        print(f\"Precision: {final_prec:.4f}\")\n",
        "        print(f\"Recall: {final_rec:.4f}\")\n",
        "        print(f\"F1 Score: {final_f1:.4f}\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_curves(fold_results: List[Dict], best_fold_idx: int):\n",
        "    \"\"\"Plot training curves for the best fold\"\"\"\n",
        "    best_fold = fold_results[best_fold_idx]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    epochs = range(1, len(best_fold['train_losses']) + 1)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax1.plot(epochs, [acc * 100 for acc in best_fold['val_accuracies']],\n",
        "             'b-', label='Validation Accuracy', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax1.set_title(f'Accuracy - Fold {best_fold[\"fold\"]}', fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss plot\n",
        "    ax2.plot(epochs, best_fold['train_losses'], 'r-', label='Training Loss', linewidth=2)\n",
        "    ax2.plot(epochs, best_fold['val_losses'], 'b-', label='Validation Loss', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Loss', fontsize=12)\n",
        "    ax2.set_title(f'Loss - Fold {best_fold[\"fold\"]}', fontsize=14, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(fold_results: List[Dict], best_fold_idx: int):\n",
        "    \"\"\"Plot confusion matrix for the best fold\"\"\"\n",
        "    best_fold = fold_results[best_fold_idx]\n",
        "\n",
        "    cm = confusion_matrix(best_fold['true_labels'], best_fold['predictions'])\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Raw counts\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                xticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                yticklabels=['Non-BacLAB', 'BacLAB'])\n",
        "    ax1.set_xlabel('Predicted', fontsize=12)\n",
        "    ax1.set_ylabel('True', fontsize=12)\n",
        "    ax1.set_title(f'Confusion Matrix (Counts) - Fold {best_fold[\"fold\"]}',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Normalized\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=ax2,\n",
        "                xticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                yticklabels=['Non-BacLAB', 'BacLAB'])\n",
        "    ax2.set_xlabel('Predicted', fontsize=12)\n",
        "    ax2.set_ylabel('True', fontsize=12)\n",
        "    ax2.set_title(f'Confusion Matrix (Normalized) - Fold {best_fold[\"fold\"]}',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution pipeline\n",
        "\n",
        "    PERFORMANCE OPTIMIZATIONS:\n",
        "    - Batch embedding extraction (64 sequences at once) → 6-12x speedup\n",
        "    - Reduced max_len from 2000 to 600 (bacteriocins are typically 50-150 aa)\n",
        "    - Pre-trained RNN weights loaded from rnn_gru.pt\n",
        "    - GPU support: automatically uses CUDA if available\n",
        "\n",
        "    FOR FASTEST RESULTS:\n",
        "    - Run on Google Colab with free GPU (T4): ~2-5 minutes for embeddings\n",
        "    - Alternatively: Run locally on Mac CPU: ~4-8 hours total\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"BacLABNet: Bacteriocin Classification using Deep Learning\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"\\n[1/7] Loading data...\")\n",
        "    # CSV has no headers: ID, Species, Sequence, Label, Empty\n",
        "    df_baclabnonbaclabdata = pd.read_csv('data_BacLAB_and_nonBacLAB.csv',\n",
        "                                          header=None,\n",
        "                                          names=['ID', 'Species', 'Sequence', 'Label', 'Empty'])\n",
        "\n",
        "    sequences = df_baclabnonbaclabdata['Sequence'].tolist()\n",
        "    labels = df_baclabnonbaclabdata['Label'].values  # 1 for BacLAB, 0 for Non-BacLAB\n",
        "\n",
        "    print(f\"Total sequences: {len(sequences)}\")\n",
        "    print(f\"BacLAB: {sum(labels)}, Non-BacLAB: {len(labels) - sum(labels)}\")\n",
        "\n",
        "    # 2. Extract k-mer features\n",
        "    print(\"\\n[2/7] Extracting k-mer features...\")\n",
        "\n",
        "    # For reproduction, load pre-computed k-mers from file\n",
        "    try:\n",
        "        kmers_df = pd.read_csv('List_kmers.csv')\n",
        "        kmers_5 = kmers_df['5-mers'].dropna().tolist()\n",
        "        kmers_7 = kmers_df['7-mers'].dropna().tolist()\n",
        "        print(\"Loaded pre-computed k-mers from file\")\n",
        "    except:\n",
        "        print(\"Computing k-mers from BacLAB sequences...\")\n",
        "        baclab_sequences = [seq for seq, lbl in zip(sequences, labels) if lbl == 1]\n",
        "        kmers_5 = get_top_kmers(baclab_sequences, k=5, top_n=100)\n",
        "        kmers_7 = get_top_kmers(baclab_sequences, k=7, top_n=100)\n",
        "\n",
        "    # Extract k-mer features for all sequences\n",
        "    features_5 = np.array([extract_kmer_features(seq, kmers_5, 5) for seq in sequences])\n",
        "    features_7 = np.array([extract_kmer_features(seq, kmers_7, 7) for seq in sequences])\n",
        "\n",
        "    print(f\"5-mer features shape: {features_5.shape}\")\n",
        "    print(f\"7-mer features shape: {features_7.shape}\")\n",
        "\n",
        "    # 3. Extract embedding features\n",
        "    print(\"\\n[3/7] Extracting embedding features...\")\n",
        "\n",
        "    # Try to load pre-computed embeddings (from Colab GPU run)\n",
        "    try:\n",
        "        embedding_features = np.load('embeddings.npy')\n",
        "        print(\"✓ Loaded pre-computed embeddings from 'embeddings.npy'\")\n",
        "        print(f\"  Shape: {embedding_features.shape}\")\n",
        "        print(\"  (Skipping embedding extraction - using cached results)\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No pre-computed embeddings found. Extracting embeddings...\")\n",
        "        print(\"  TIP: For faster processing, run 'colab_embedding_extraction.py' on Google Colab GPU\")\n",
        "\n",
        "        # Detect device (GPU if available, otherwise CPU)\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {device.upper()}\")\n",
        "\n",
        "        # Initialize embedding model\n",
        "        embedding_model = EmbeddingRNN(vocab_size=21, embedding_dim=128)\n",
        "\n",
        "        # Load pre-trained RNN weights from published model\n",
        "        try:\n",
        "            state_dict = torch.load('rnn_gru.pt', map_location=device)\n",
        "            embedding_model.load_state_dict(state_dict)\n",
        "            print(\"✓ Loaded pre-trained RNN model from 'rnn_gru.pt'\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Could not load pre-trained model: {e}\")\n",
        "            print(\"  Using randomly initialized embedding model instead\")\n",
        "\n",
        "        # Extract embeddings with batching (6-12x faster than 1-by-1)\n",
        "        start_time = time.time()\n",
        "        embedding_features = extract_embedding_features(\n",
        "            sequences,\n",
        "            embedding_model,\n",
        "            max_len=600,  # Reduced from 2000 (most bacteriocins are 50-150 aa)\n",
        "            batch_size=64,  # Process 64 sequences at once\n",
        "            device=device\n",
        "        )\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Embedding features shape: {embedding_features.shape}\")\n",
        "        print(f\"Extraction time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
        "        print(f\"Speed: {len(sequences)/elapsed_time:.1f} sequences/second\")\n",
        "\n",
        "        # Save for future use\n",
        "        np.save('embeddings.npy', embedding_features)\n",
        "        print(\"✓ Saved embeddings to 'embeddings.npy' for future runs\")\n",
        "\n",
        "    # 4. Concatenate features (5-mers + 7-mers + EV)\n",
        "    print(\"\\n[4/7] Concatenating features...\")\n",
        "    features = np.concatenate([features_5, features_7, embedding_features], axis=1)\n",
        "    print(f\"Final features shape: {features.shape}\")\n",
        "    print(f\"Input dimension: {features.shape[1]} (100 + 100 + 128 = 328)\")\n",
        "\n",
        "    # 5. Perform k-fold cross-validation\n",
        "    print(\"\\n[5/7] Starting k-fold cross-validation (k=30)...\")\n",
        "    fold_results = kfold_cross_validation(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        k=30,\n",
        "        epochs=75,\n",
        "        batch_size=40,\n",
        "        learning_rate=2.5e-5\n",
        "    )\n",
        "\n",
        "    # 6. Analyze results\n",
        "    print(\"\\n[6/7] Analyzing results...\")\n",
        "\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Fold': r['fold'],\n",
        "        'Loss (%)': r['loss'],\n",
        "        'Accuracy (%)': r['accuracy'],\n",
        "        'Precision': r['precision'],\n",
        "        'Recall': r['recall'],\n",
        "        'F1 Score': r['f1_score']\n",
        "    } for r in fold_results])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CROSS-VALIDATION RESULTS (k=30)\")\n",
        "    print(\"=\"*70)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AVERAGE METRICS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Loss: {results_df['Loss (%)'].mean():.2f}%\")\n",
        "    print(f\"Accuracy: {results_df['Accuracy (%)'].mean():.2f}%\")\n",
        "    print(f\"Precision: {results_df['Precision'].mean():.4f}\")\n",
        "    print(f\"Recall: {results_df['Recall'].mean():.4f}\")\n",
        "    print(f\"F1 Score: {results_df['F1 Score'].mean():.4f}\")\n",
        "\n",
        "    # Find best fold (Fold 22 in paper)\n",
        "    best_fold_idx = results_df['Accuracy (%)'].idxmax()\n",
        "    best_fold = fold_results[best_fold_idx]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"BEST FOLD: Fold {best_fold['fold']}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Loss: {best_fold['loss']:.2f}%\")\n",
        "    print(f\"Accuracy: {best_fold['accuracy']:.2f}%\")\n",
        "    print(f\"Precision: {best_fold['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_fold['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {best_fold['f1_score']:.4f}\")\n",
        "\n",
        "    # 7. Visualize results\n",
        "    print(\"\\n[7/7] Generating visualizations...\")\n",
        "    plot_training_curves(fold_results, best_fold_idx)\n",
        "    plot_confusion_matrix(fold_results, best_fold_idx)\n",
        "\n",
        "    # Save best model\n",
        "    torch.save(best_fold['model_state'], 'best_model_fold22.pt')\n",
        "    print(\"\\nBest model saved as 'best_model_fold22.pt'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}